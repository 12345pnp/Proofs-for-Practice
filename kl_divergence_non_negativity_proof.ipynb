{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h3>Proof that KL Divergence is always non-negative</h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$$ KL (P || Q) = \\int_{-\\infty}^{\\infty} p(x) \\log\\\n",
        "\\dfrac{p(x)}{q(x)} dx $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h4> Jensen's Inequality </h4>\n",
        "\n",
        "For convex functions:\n",
        "\n",
        "$$ f(\\mathbb{E}[X]) \\leq \\mathbb{E}[f(X)]$$\n",
        "\n",
        "For concave functions:\n",
        "\n",
        "$$ f(\\mathbb{E}[X]) \\geq \\mathbb{E}[f(X)]$$\n",
        "\n",
        "Note: $\\log$ is a concave function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$$- KL (P || Q) = \\int_{-\\infty}^{\\infty} p(x) \\log\\\n",
        "\\dfrac{q(x)}{p(x)} dx = \\mathbb{E}_p \\left[ \\log\\\n",
        "\\dfrac{q(x)}{p(x)} \\right] \\leq \\log \\left( \\mathbb{E}_p  \\left[ \\dfrac{q(x)}{p(x)}  \\right]\\right)= \\log \\left( \\int_{-\\infty}^{\\infty} \\cancel{p(x)} \\dfrac{q(x) }{\\cancel{p(x)}} dx \\right)= \\log(1) = 0$$\n",
        "\n",
        "$$ \\implies  KL (P || Q) \\geq 0 $$"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "test_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
